{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# recommended Python3 version >= 3.5\n",
    "print('Python version: {}'.format(platform.sys.version))\n",
    "\n",
    "# data-science & processing tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import h5py\n",
    "\n",
    "# progress bar\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# plotting utilities\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# required TensorFlow version >= 2.0.0\n",
    "tf_version = tf.__version__\n",
    "print('TensorFlow version: {}'.format(tf_version))\n",
    "assert int(tf_version[0]) >= 2, \"Tensorflow version must be >= 2.0\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#seed random numbers for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "print('\\nImports Complete.')\n",
    "\n",
    "%matplotlib inline\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_names = np.array(pd.read_csv('experiments_order.txt', header = None)[0]) #read in the names of expression conditions \n",
    "\n",
    "#parameters for finetuning\n",
    "COND_IND = list(cond_names).index('Heat') #index for the first condition to try and predict with the CNN\n",
    "KERNEL_SIZE = 11 #size of the kernel to predict\n",
    "SEQ_LENGTH = 1000 #length of upstream sequence from which to predict, must be 20, 50 or 100\n",
    "dropout_rate = 0.1\n",
    "l2_lambda = 0.001\n",
    "learning_rate = 1e-04\n",
    "METRICS = [\n",
    "      K.metrics.TruePositives(name='tp'),\n",
    "      K.metrics.FalsePositives(name='fp'),\n",
    "      K.metrics.TrueNegatives(name='tn'),\n",
    "      K.metrics.FalseNegatives(name='fn'), \n",
    "      K.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      K.metrics.Precision(name='precision'),\n",
    "      K.metrics.Recall(name='recall'),\n",
    "      K.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# One-hot encode DNA sequence data\n",
    "def one_hot_encode(seq):\n",
    "    one_hot_dict = {'A': np.array([1,0,0,0]),\n",
    "                   'C': np.array([0,1,0,0]),\n",
    "                   'G': np.array([0,0,1,0]),\n",
    "                   'T': np.array([0,0,0,1])}\n",
    "    return np.array([one_hot_dict[x] for x in seq])\n",
    "\n",
    "# Return the one-hot encoded sequence and corresponding expression data for a given upstream sequence length\n",
    "# The upstream sequence length must already be present in the data file\n",
    "def process_seqs_by_length (data_file, seq_length, expression_column = 'Norm Expression'):\n",
    "    #read in data\n",
    "    data = pd.read_csv(data_file)#['Norm Expression'][1]\n",
    "    try:\n",
    "        data = data.dropna(subset=[str(seq_length) + 'bp upstream sequence']) #removes genes without upstream sequences\n",
    "    except:\n",
    "        raise Exception (\"Can't get data for sequence of length %s\" % seq_length)\n",
    "    expressions = np.array(data[expression_column]) # imports numerical data as strings\n",
    "    \n",
    "    seq_strings = data[str(seq_length) + 'bp upstream sequence'].values #INPUT, upstream sequences\n",
    "    try:\n",
    "        labels = np.array([[float(expr) for expr in sample[1:-1].split(', ')] for sample in expressions]) #convert expression data into arrays of floats\n",
    "    except:\n",
    "        labels = np.array([[float(expr) for expr in re.split('\\n ?| +', sample[1:-1].strip()) if expr != ''] for sample in expressions])\n",
    "    bin_labels = (labels > 0).astype('int64') # binarized labels (1 if expressed more than average, 0 if expressed less than average)\n",
    "    seq_one_hot = np.array([one_hot_encode(seq) for seq in seq_strings])\n",
    "    return seq_one_hot, labels, bin_labels\n",
    "\n",
    "# Split the data seet into training and validation and return the input data and labels\n",
    "def train_val_split (seq_one_hot, labels, cond_ind = COND_IND, train_frac = 0.8):\n",
    "    \n",
    "    arr_shape = seq_one_hot.shape\n",
    "    data_split = int(arr_shape[0]*train_frac)\n",
    "    \n",
    "    if cond_ind:\n",
    "        labels = labels[:, COND_IND] #pick which growth condition to predict growth for\n",
    "\n",
    "    rand_ind = np.random.choice(range(arr_shape[0]), arr_shape[0], replace = False)\n",
    "    train_inds = rand_ind[:data_split]\n",
    "    val_inds = rand_ind[data_split:]\n",
    "\n",
    "    t_shape = seq_one_hot[train_inds,:,:].shape\n",
    "    v_shape = seq_one_hot[val_inds,:,:].shape\n",
    "\n",
    "    x_train = tf.reshape(seq_one_hot[train_inds,:,:], (t_shape[0],1,t_shape[1],t_shape[2]))\n",
    "    y_train = labels[train_inds]\n",
    "    x_val = tf.reshape(seq_one_hot[val_inds,:,:], (v_shape[0],1,v_shape[1],v_shape[2]))\n",
    "    y_val = labels[val_inds]\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for building a more complex or a simpler CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a Keras Sequential model with 3 convolutional layers, 2 hidden fully connected layers, and\n",
    "# a prediction layer\n",
    "def build_comp_model(input_shape, l2_lambda, dropout_rate, output_size = 1, output_activation='sigmoid') :\n",
    "    model = K.Sequential()\n",
    "    model.add(K.layers.Conv2D(\n",
    "        filters=30, kernel_size=(1, 20), strides=(1, 1), padding='valid', \n",
    "        kernel_regularizer='l2', activation = 'relu', input_shape = (input_shape)\n",
    "    ))\n",
    "    model.add(K.layers.MaxPool2D(pool_size=(1,3)))\n",
    "    model.add(K.layers.Conv2D(\n",
    "        filters=20, kernel_size=(1, 6), strides=(1, 1), padding='valid', \n",
    "        kernel_regularizer='l2', activation = 'relu', input_shape = (x_train.shape[1:])\n",
    "    ))\n",
    "    \n",
    "    model.add(K.layers.MaxPool2D(pool_size=(1,3)))\n",
    "    model.add(K.layers.Conv2D(\n",
    "        filters=20, kernel_size=(1, 4), strides=(1, 1), padding='valid', \n",
    "        kernel_regularizer='l2', activation = 'relu', input_shape = (x_train.shape[1:])\n",
    "    ))\n",
    "    model.add(K.layers.MaxPool2D(pool_size=(1,3)))\n",
    "\n",
    "    model.add(K.layers.Flatten())\n",
    "    model.add(K.layers.Dense(500, activation = 'relu', kernel_regularizer=K.regularizers.l2(l2_lambda)))\n",
    "    model.add(K.layers.Dropout(dropout_rate))\n",
    "    model.add(K.layers.Dense(500, activation = 'relu', kernel_regularizer=K.regularizers.l2(l2_lambda)))\n",
    "    model.add(K.layers.Dropout(dropout_rate))\n",
    "    model.add(K.layers.Dense(output_size, activation=output_activation))\n",
    "    return model\n",
    "\n",
    "# Returns a Keras Sequential model with 1 convolutional layer, 1 hidden fully connected layers, and\n",
    "# a prediction layer\n",
    "def build_simp_model(input_shape, l2_lambda, dropout_rate, output_size = 1, output_activation='sigmoid', fc_layers=500) :\n",
    "    model = K.Sequential()\n",
    "    model.add(K.layers.Conv2D(\n",
    "        filters=5, kernel_size=(1, 5), strides=(1, 1), padding='valid', \n",
    "        kernel_regularizer='l2', activation = 'relu', input_shape = (input_shape)\n",
    "    ))\n",
    "    model.add(K.layers.MaxPool2D(pool_size=(1,3)))\n",
    "    model.add(K.layers.Flatten())\n",
    "    model.add(K.layers.Dense(fc_layers, activation = 'relu', kernel_regularizer=K.regularizers.l2(l2_lambda)))\n",
    "    model.add(K.layers.Dropout(dropout_rate))\n",
    "    model.add(K.layers.Dense(output_size, activation=output_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for grid search and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search across l2 coefficients, optimizer learning rates, and upstream sequence lengths\n",
    "# The loss and accuracy functions can be customized\n",
    "def grid_search(build_model,\n",
    "                reg_coeffs,\n",
    "                learning_rates,\n",
    "                seq_lengths,\n",
    "                dropout_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                training_data,\n",
    "                val_data, \n",
    "                metrics,\n",
    "                loss = 'binary_crossentropy',\n",
    "                loss_name = 'val_loss',\n",
    "                acc_name = 'val_accuracy',\n",
    "               **kwargs):\n",
    "\n",
    "    losses = []\n",
    "    accs = []\n",
    "    hyperparam_combos = list(\n",
    "        it.product(seq_lengths, reg_coeffs, learning_rates))\n",
    "    \n",
    "    \n",
    "    for seq_length, reg_coeff, learning_rate in hyperparam_combos:\n",
    "        x_train, y_train = training_data[seq_length]\n",
    "        x_val, y_val = val_data[seq_length]\n",
    "        model = build_model(x_train.shape[1:], reg_coeff, dropout_rate, **kwargs)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "        \n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "        history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=(x_val, y_val))\n",
    "        best_loss = min(history.history[loss_name])\n",
    "        best_acc = max(history.history[acc_name])\n",
    "        \n",
    "        losses.append([seq_length, \n",
    "                       reg_coeff, \n",
    "                       learning_rate, \n",
    "                       best_loss])\n",
    "        \n",
    "        accs.append([seq_length, \n",
    "                       reg_coeff, \n",
    "                       learning_rate, \n",
    "                       best_acc])\n",
    "        \n",
    "        \n",
    "    losses_df = pd.DataFrame(losses, \n",
    "                             columns=['seq_length',\n",
    "                                      'L2 lambda',\n",
    "                                      'learning rate',\n",
    "                                      loss_name])    \n",
    "    accs_df = pd.DataFrame(accs, \n",
    "                             columns=['seq_length',\n",
    "                                      'L2 lambda',\n",
    "                                      'learning rate',\n",
    "                                       acc_name])    \n",
    "    return losses_df, accs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get training and validation data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_one_hot, labels, bin_labels  = process_seqs_by_length ('cleaned_data.csv', SEQ_LENGTH)\n",
    "x_train, y_train, x_val, y_val = train_val_split (seq_one_hot, bin_labels)\n",
    "\n",
    "print ('X train %s' % x_train.shape)\n",
    "print ('Y train %s' % y_train.shape)\n",
    "print ('X validation %s' % x_val.shape)\n",
    "print ('Y validation %s'% y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test building and training complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-04\n",
    "model = build_comp_model(x_train.shape[1:], l2_lambda, dropout_rate)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test fitting model for a single expression point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# Fit model on training data')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a grid search on hyperparameters for the simpler model\n",
    "The complex model did not perform better than random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to try\n",
    "reg_coeffs = [0, 1e-02, 1e-01]\n",
    "learning_rates = [1e-05, 1e-04, 1e-03]\n",
    "seq_lengths = [20, 50, 100, 200, 500, 1000]\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "training_data = {}\n",
    "val_data = {}\n",
    "for length in seq_lengths:\n",
    "    seq_one_hot, _, bin_labels  = process_seqs_by_length ('cleaned_data.csv', length)\n",
    "    x_train, y_train, x_val, y_val = train_val_split (seq_one_hot, bin_labels)\n",
    "    training_data[length] = (x_train, y_train)\n",
    "    val_data[length] = (x_val, y_val)\n",
    "\n",
    "loss_df, acc_df = grid_search(build_simp_model,\n",
    "                reg_coeffs,\n",
    "                learning_rates,\n",
    "                seq_lengths,\n",
    "                dropout_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                training_data,\n",
    "                val_data, \n",
    "                METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pivot = (acc_df\n",
    "                     .pivot_table(values=['val_accuracy'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['seq_length', 'learning rate']))\n",
    "acc_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pivot = (loss_df\n",
    "                     .pivot_table(values=['val_loss'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['seq_length', 'learning rate']))\n",
    "loss_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using hyperparameters from the grid search, train a model for each experimental growth condition to see if there are differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l2 = 0\n",
    "best_lr = 0.001\n",
    "best_seq_length = 200\n",
    "\n",
    "best_seq_one_hot, _, best_bin_labels  = process_seqs_by_length ('cleaned_data.csv', best_seq_length)\n",
    "\n",
    "#unpacker = {0:np.array([1,0]), 1:np.array([0,1])}\n",
    "#unpacked_bin_label = np.array([np.array([unpacker[x] for x in condition]) for condition in best_bin_labels])\n",
    "x_train, y_train, x_val, y_val = train_val_split(best_seq_one_hot, best_bin_labels, cond_ind = None)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = best_lr)\n",
    "\n",
    "best_val_acc = [] #for each condition, stores the validation loss from the best model trained for that condition\n",
    "best_val_loss = []\n",
    "best_val_auc = []\n",
    "for i in range(len(y_train[0])):\n",
    "    model_2 = build_simp_model(x_train.shape[1:], best_l2, dropout_rate, fc_layers=100)\n",
    "    model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=METRICS)\n",
    "    \n",
    "    history_2 = model_2.fit(x_train, y_train[:, i],\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        validation_data=(x_val, y_val[:, i]))\n",
    "    \n",
    "    best_val_acc.append(np.max(history_2.history['val_accuracy']))\n",
    "    best_val_loss.append(np.min(history_2.history['val_loss']))\n",
    "    best_val_auc.append(np.max(history_2.history['val_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ls = np.flip(np.argsort(best_val_auc))\n",
    "print ('Best predicted condition names and accuracy: ')\n",
    "print (dict(zip((list(cond_names[sort_ls[:10]])), list(np.array(best_val_auc)[sort_ls[:10]]))))\n",
    "print \n",
    "print ('Worst predicted condition names and accuracy: ')\n",
    "print (dict(zip((list(cond_names[sort_ls[-9:]])), list(np.array(best_val_auc)[sort_ls[-9:]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize class imbalance across the different experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_overexpressed = np.sum(y_val, axis=0)/ y_val.shape[0] # per condition, fraction of genes that are overexpressed\n",
    "plt.hist(frac_overexpressed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show that any difference in accuracy between models is due to class imbalance in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_one_category = np.array(list(map(lambda x : max(x, 1-x), frac_overexpressed)))\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(frac_one_category, best_val_acc)\n",
    "plt.xlabel('Fraction of entries with the same label', fontsize = 20)\n",
    "plt.ylabel('Fraction of genes correctly labeled in a condition', fontsize = 20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "#plt.savefig('figures/acc-label_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.hist(best_val_auc)\n",
    "plt.xlabel(\"Estimated Area under the ROC curve\", fontsize=20)\n",
    "plt.ylabel(\"Number of Conditions\", fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "#plt.savefig('figures/AUC_hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test training a single simple CNN regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-04\n",
    "\n",
    "reg_metrics = [\n",
    "      K.metrics.MeanSquaredError(name='accuracy')\n",
    "]\n",
    "\n",
    "seq_one_hot, raw_expr, _ = process_seqs_by_length ('cleaned_data.csv', 100, expression_column = 'Expression')\n",
    "\n",
    "x_train, y_train, x_val, y_val = train_val_split(seq_one_hot, raw_expr, cond_ind=None)\n",
    "\n",
    "model_reg = build_simp_model(x_train.shape[1:], l2_lambda, dropout_rate, len(labels[0]), None, 1000)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "model_reg.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "reg_history = model_reg.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a grid search for hyperparameters for the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to try\n",
    "reg_coeffs = [0, 1e-02, 1e-01]\n",
    "learning_rates = [1e-04, 1e-03]\n",
    "seq_lengths = [20, 50, 100, 200, 500, 1000]\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "\n",
    "reg_training_data = {}\n",
    "reg_val_data = {}\n",
    "for length in seq_lengths:\n",
    "    seq_one_hot, raw_expr, _ = process_seqs_by_length ('cleaned_data.csv', length, expression_column = 'Expression')\n",
    "    x_train, y_train, x_val, y_val = train_val_split (seq_one_hot, raw_expr, cond_ind=None)\n",
    "    reg_training_data[length] = (x_train, y_train)\n",
    "    reg_val_data[length] = (x_val, y_val)\n",
    "    \n",
    "loss_df, _ = grid_search(build_simp_model,\n",
    "                    reg_coeffs,\n",
    "                    learning_rates,\n",
    "                    seq_lengths,\n",
    "                    dropout_rate,\n",
    "                    batch_size,\n",
    "                    num_epochs,\n",
    "                    reg_training_data,\n",
    "                    reg_val_data, \n",
    "                    reg_metrics,\n",
    "                    loss = 'mean_squared_error',\n",
    "                    loss_name = 'val_accuracy',\n",
    "                    acc_name = 'val_loss',\n",
    "                    output_size = len(labels[0]), output_activation=None, fc_layers=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df['Mean Squared Error'] = loss_df['val_accuracy']\n",
    "reg_loss_pivot = (loss_df\n",
    "                     .pivot_table(values=['Mean Squared Error'],\n",
    "                                  columns=['L2 lambda'],\n",
    "                                  index=['seq_length', 'learning rate']))\n",
    "reg_loss_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l2 = 0.01\n",
    "best_lr = 0.001\n",
    "best_len = 500\n",
    "x_train, y_train = reg_training_data[best_len]\n",
    "x_val, y_val = reg_val_data[best_len]\n",
    "\n",
    "model_reg = build_simp_model(x_train.shape[1:], best_l2, dropout_rate, len(labels[0]), None, 500)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = best_lr)\n",
    "model_reg.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error', K.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_history_opt = model_reg.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=20,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_hat = model_reg.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression model accurately captures the mean of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(np.mean(y_val, axis=0), label = 'Mean experimental gene expression')\n",
    "plt.plot(np.mean(y_val_hat, axis=0), label='Mean predicted gene expression')\n",
    "plt.xlabel('Experimental condition index', fontsize=18)\n",
    "plt.ylabel('Experimental condition index', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig('figures/mean_expression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression model fails to make meaningful predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('RMSE', reg_history_opt.history['val_root_mean_squared_error'][-1] )\n",
    "print ('NRMSE', reg_history_opt.history['val_root_mean_squared_error'][-1]/np.mean(y_val) )\n",
    "print ('normalized mean standard deviation', np.mean(np.sqrt(np.mean(np.square(y_val - y_val_hat), axis=0))/np.mean(y_val) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (8,16):\n",
    "    plt.figure()\n",
    "    plt.plot(y_val[i], label='real')\n",
    "    plt.plot(y_val_hat[i], label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python rdkit",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
